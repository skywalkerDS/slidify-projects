---
title: "Predict the Manner of Exercise"
author: "Randy Qin"
date: "Wednesday, November 19, 2014"
output: html_document
---

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to quantify not only how much of a particular activity wearers do, but how well they do it. Our goal here will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways, to predict the manner in which people did the exercise. 

Details are here: <http://groupware.les.inf.puc-rio.br/har>.

### Source Data 


The training data for this project are available here: 

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here: 

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

We would like to thank Groupware HAR for the generous offer allowing their data to be used for this study. 

### Data Exploration

As defined, data reflects 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

```{r}
pml_data <- read.csv("pml-training.csv", na="", as.is=T)
dim(pml_data)
```

This is a fairly large dataset, with 160 attributes.  Since we are interested in measurements from sensors, we want to drop the non-numeric data columns and make the categoy varible 'classe' as the output.

```{r}
# catch the result classes, the last column 
classe <- pml_data[,ncol(pml_data)]
# drop any non-numeric columns
col_nums <- sapply(pml_data, is.numeric)
pml <- pml_data[,col_nums]
# only keep the measures
pml <- pml[,-c(1:4)]
# keep the columns, then add in output: classe 
col_ds <- colnames(pml)
pml <- cbind(pml, classe)
colnames(pml)
summary(pml$classe)
```
As the result, predictors now stand at 52.

### Build the model

All models start with traning and test sets preparation.  Considering the large size of dataset, we choose to use a training set with low thousands of records, a 15%-85% partition seems to be a good start.

```{r, message=FALSE}
library(caret)

set.seed(331)
inTrain <- createDataPartition(y=pml$classe, p=0.15, list=F)
training <- pml[inTrain,]
testing <- pml[-inTrain,]
```
```{r}
dim(training);dim(testing)
```

This ~3000 training set is used in model building and benchmarking below.

#### Cross validation and sample error

For CART model, we use the default 25-reps bootstrap cross validation and 10-fold cross validation (repeating 3 times): 

```{r}
library(rpart)
tree <- train(classe ~ ., data=training, method='rpart')

cvCtrl <- trainControl(method="repeatedcv", repeats=3)
tree2 <- train(classe ~ ., data=training, method='rpart', tuneLength=30, trControl=cvCtrl)
```

To save space, the tree results are not included here but can be reproduced easily.  The training with 10 fold cross-validation takes longer than bootstrap to complete, in about 30 seconds, but the accuracy increased to 81% from bootstrap's 45%. 

#### The choices of model

While CART model's 81% accuracy is pretty encouraging, we also tried multiple popular predication models. The results of accuray and performance are:  

```{r, eval=FALSE}
# LDA
fit_lda <- train(classe ~ . , method='lda', data=training)
# Naive Bayes
library(klaR)
fit_nb <- train(classe ~ . , method='nb', data=training)
# Boosting
library(gbm)
fit_bt <- train(classe ~ . , method='gbm', data=training, verbose=F)
```

What we have found during model building:

+ LDA is fast, accuracy is 70% in 8 seconds;
+ Naive Bayes has very similar accuracy as LDA, 71%, but it takes 830 seconds(!!!);
+ GBM is really good, accuracy 94%, training takes 360 seconds; and
+ CART accuracy 81% (10-fold, 3 reps), training takes 30 seconds

The model Decision tree with boosting (GBM) clearly is leading pack in our study.  But before we claim GBM as our pick of model, let us check out the random forest:

```{r, message=FALSE}
library(randomForest)
rf_tree <- randomForest(classe ~ . , data=training, prox=T)
```
```{r}
rf_tree
```

Very impressive! 3.6% sample error after 24 seconds of training.

As a summary:

Model | Accuracy (Training set size 2946) | Training time (second)
--- | --- | ---
CART (25 reps bootstrap) | 45% | 19
CART (10-fold cross-validation, 3 reps) | 81% | 36 
LDA | 70% | 8
Naive BAyes | 71% | 830
GBM | 94% | 360
Random Forest | 96% | 24 

We have our winner model, Random Forest.

### Prediction

With the 96% accuracy, there is no need to increase the size of trainging set of our Random Forest model.   

Because the original dataset was transformed when we chose data fields in model bulding, so for real prediction the test data must be in the same format as the training set.  We take advantage of the column list kept when we prepared the training set, and use it to select the columns and format the real test data:

```{r}
pml_real <- read.csv("pml-testing.csv")
pml_cases <- subset(pml_real, select=col_ds)
```

The random forest model works very well in prediction for the 20 cases in our study, and the results have been submitted for evaluation (turn out to be 100% correct!) :

```{r}
pml_pred <- predict(rf_tree, pml_cases)
# hide the results
#pml_pred
```

### Conclusion

For categorized data such as fitness exercise data, random forest is a good choice of model for prediction.  With quite a small training set (~3000 records), random forest can predict manner of exercise with 96% accuracy very fast (24 seconds).

```{r, results='hide', echo=FALSE, eval=FALSE}
### Appendix: Project submission
answers = as.character(pml_pred)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

```